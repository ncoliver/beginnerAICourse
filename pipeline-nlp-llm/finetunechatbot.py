# -*- coding: utf-8 -*-
"""finetuneChatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DKJoJkaLbUiPq4GMRnq7B7UwAOfblfFI
"""

!pip install transformers datasets peft accelerate gradio

# ‚úÖ 2Ô∏è‚É£ Imports
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
from datasets import load_dataset
from peft import LoraConfig, get_peft_model
import torch
import gradio as gr

# ‚úÖ 3Ô∏è‚É£ Load model and tokenizer
model_name = "tiiuae/falcon-1b-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token   # important for padding

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float32,
    device_map="cpu"
)

# ‚úÖ 4Ô∏è‚É£ Load and preprocess dataset
dataset = load_dataset("json", data_files="basketball.json")

def tokenize(example):
    # Concatenate question and answer text
    text = example["question"] + "\n" + example["answer"] + tokenizer.eos_token
    tokenized = tokenizer(
        text,
        truncation=True,
        max_length=512,
        padding="max_length"
    )
    # Add labels for supervised fine-tuning
    tokenized["labels"] = tokenized["input_ids"].copy()
    return tokenized

tokenized_ds = dataset.map(tokenize)

# ‚úÖ 5Ô∏è‚É£ Configure LoRA for parameter-efficient fine-tuning
lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)
model = get_peft_model(model, lora_config)

# ‚úÖ 6Ô∏è‚É£ Define training arguments
training_args = TrainingArguments(
    output_dir="./chatbot_model",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    num_train_epochs=2,  # start small; increase after verifying
    learning_rate=2e-4,
    fp16=True,
    logging_steps=10,
    save_strategy="epoch",
    optim="paged_adamw_8bit"  # memory-efficient optimizer
)

# ‚úÖ 7Ô∏è‚É£ Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_ds["train"]
)

# ‚úÖ 8Ô∏è‚É£ Train
trainer.train()

# ‚úÖ Save the fine-tuned LoRA adapter
model.save_pretrained("./chatbot_model")
print("‚úÖ LoRA adapter saved to ./chatbot_model")

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from peft import PeftModel

base_model_name = "tiiuae/falcon-1b-instruct"

# Load base model + tokenizer
tokenizer = AutoTokenizer.from_pretrained(base_model_name)
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_name, device_map="auto", torch_dtype="auto"
)

# Load adapter (now exists!)
model = PeftModel.from_pretrained(base_model, "./chatbot_model")

pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, device_map="auto")

print(pipe("Explain systematic basketball:", max_new_tokens=150)[0]["generated_text"])

def chatbot(prompt, chat_history=[]):
    """Generate model response given user input and prior conversation."""
    history_text = "\n".join([f"User: {u}\nAssistant: {a}" for u, a in chat_history])
    full_prompt = f"{history_text}\nUser: {prompt}\nAssistant:"

    output = pipe(
        full_prompt,
        max_new_tokens=200,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        repetition_penalty=1.1
    )[0]["generated_text"]

    # Extract only the assistant part
    response = output.split("Assistant:")[-1].strip()
    chat_history.append((prompt, response))
    return chat_history, chat_history

def save_feedback(rating, comment):
    with open("feedback.csv", "a") as f:
        f.write(f"{rating},{comment}\n")
    return "‚≠ê Thanks for your feedback!"

with gr.Blocks() as demo:
    gr.Markdown("# Teaching Chatbot with Feedback")
    chat = gr.Chatbot()
    text = gr.Textbox(label="Ask me something!")
    text.submit(lambda m, h: (h + [(m, f"Response: {m}")], ""), [text, chat], [chat, text])

    gr.Markdown("## üí¨ Rate your experience")
    rating = gr.Slider(1, 5, value=5, step=1, label="Rate (1=Poor, 5=Excellent)")
    comment = gr.Textbox(label="Optional comment")
    submit = gr.Button("Submit")
    output = gr.Textbox(label="Status", interactive=False)

    submit.click(save_feedback, [rating, comment], output)

demo.launch()

